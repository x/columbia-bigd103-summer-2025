{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1g1eZPmcfbXYdAQa8ah91XHId-OJqkDmj","timestamp":1753991865453}],"authorship_tag":"ABX9TyP4YHLCUtCmYzdNzOLSxta9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Using TFIDF to Analyze Song Lyrics\n","\n","In this exercise you'll discover what makes a song unique by finding its most distinctive words using TF-IDF.\n","\n","## Assignment Overview\n","\n","You will:\n","1. Load the lyrics dataset into pandas\n","2. Select a target song\n","3. Preprocess the lyrics (turn text into lists of words)\n","4. Calculate TF-IDF to find what words make this song special\n"],"metadata":{"id":"IDTMIBU3-Bbe"}},{"cell_type":"markdown","source":["# Part 1: Setup and import"],"metadata":{"id":"hFoDORmv1PWP"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"Se22q_TX94_S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754018630176,"user_tz":240,"elapsed":838,"user":{"displayName":"Devon Peticolas","userId":"01655597089936748753"}},"outputId":"435c3d99-6283-4097-bbb0-cd37de884960"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Index(['Artist', 'Title', 'Album', 'Year', 'Date', 'Lyric'], dtype='object')\n","Total songs: 5981\n","     Artist            Title             Album    Year        Date  \\\n","0  Dua Lipa        New Rules          Dua Lipa  2017.0  2017-06-02   \n","1  Dua Lipa  Donâ€™t Start Now  Future Nostalgia  2019.0  2019-11-01   \n","2  Dua Lipa            IDGAF          Dua Lipa  2017.0  2017-06-02   \n","\n","                                               Lyric  \n","0  one one one one one   talkin' in my sleep at n...  \n","1  if you don't wanna see me   did a full 80 craz...  \n","2  you call me all friendly tellin' me how much y...  \n"]}],"source":["# JUST RUN THIS, no changes needed\n","\n","from google.colab import drive\n","import pandas as pd\n","import math\n","import re\n","\n","drive.mount('/content/gdrive')\n","df = pd.read_csv('/content/gdrive/MyDrive/datasets/lyrics.csv')\n","\n","# FIXUP DATA\n","df[\"Title\"] = df[\"Title\"].str.replace(\"\\u200b\", \"\")\n","\n","# Look at the data structure\n","print(df.columns)\n","print(f\"Total songs: {len(df)}\")\n","print(df.head(3))"]},{"cell_type":"markdown","source":["# Part 2: Define our preprocessor"],"metadata":{"id":"fmU0c2oG1TGS"}},{"cell_type":"code","source":["# JUST RUN THIS, no changes needed\n","\n","STOP_WORDS = {\n","    \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"by\", \"for\", \"from\",\n","    \"has\", \"he\", \"in\", \"is\", \"it\", \"its\", \"of\", \"on\", \"that\", \"the\",\n","    \"to\", \"was\", \"were\", \"will\", \"with\", \"i\", \"you\", \"we\", \"they\",\n","    \"me\", \"my\", \"your\", \"our\", \"their\", \"him\", \"her\", \"she\"\n","}\n","\n","def preprocess(text):\n","    \"\"\"Convert text to a list of lowercase words, removing stop words\"\"\"\n","    # Convert to lowercase first\n","    text = text.lower()\n","\n","    # Split on punctuation and whitespace\n","    tokens = re.split(r\"[,\\.\\!\\?\\s]+\", text)\n","\n","    # Keep only non-empty tokens that aren't stop words\n","    processed_tokens = []\n","    for token in tokens:\n","        if token and token not in STOP_WORDS:\n","            processed_tokens.append(token)\n","\n","    return processed_tokens\n","\n","# Test it\n","test_text = \"Hello! How are you doing today?\"\n","print(preprocess(test_text))  # Should print: ['hello', 'how', 'doing', 'today']"],"metadata":{"id":"AVzpQ6UUuuEA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754018630182,"user_tz":240,"elapsed":17,"user":{"displayName":"Devon Peticolas","userId":"01655597089936748753"}},"outputId":"354222bb-9987-4426-9c41-17a4d8016ee8"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["['hello', 'how', 'doing', 'today']\n"]}]},{"cell_type":"markdown","source":["# Part 3: Get Lyrics\n","Write a get_song_lyrics function\n"],"metadata":{"id":"OmMy9Via1Us5"}},{"cell_type":"code","source":["def get_song_lyrics(lyrics_df, artist, title):\n","    artist_df = lyrics_df[lyrics_df[\"Artist\"] == artist]\n","    title_df = artist_df[artist_df[\"Title\"] == title]\n","    return title_df['Lyric'].values[0]\n","\n","# Test your function\n","artist = \"Eminem\"  # Change to your choice!\n","title = \"Rap God\"  # Change to your choice!\n","\n","target_lyrics = get_song_lyrics(df, artist, title)\n","print(f\"Found lyrics for {artist} - {title}\")\n","print(f\"First 200 chars: {target_lyrics[:200]}...\")\n"],"metadata":{"id":"nXi7UYQOGODc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754018630183,"user_tz":240,"elapsed":10,"user":{"displayName":"Devon Peticolas","userId":"01655597089936748753"}},"outputId":"8729ca56-0f92-4d2a-eac9-57de8e22200b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Found lyrics for Eminem - Rap God\n","First 200 chars: look i was gonna go easy on you not to hurt your feelings but i'm only going to get this one chance six minutes six minutes something's wrong i can feel it six minutes slim shady you're on just a feel...\n"]}]},{"cell_type":"markdown","source":["\n","# Part 4: Calculate Term Frequency\n","Write a function to calculate word frequencies"],"metadata":{"id":"1q5fhnYD1XaX"}},{"cell_type":"code","source":["def calculate_term_frequency(text):\n","    term_freq = {}\n","\n","    # FIRST, preprocess the text\n","    processed_text = preprocess(text)\n","\n","    for word in processed_text:\n","        term_freq[word] = term_freq.get(word, 0) + 1\n","\n","    return term_freq\n","\n","# Test your function by running the below\n","tf = calculate_term_frequency(target_lyrics)\n","print(f\"Unique words: {len(tf)}\")\n","print(\"Top 5 words:\", sorted(tf.items(), key=lambda x: x[1], reverse=True)[:5])"],"metadata":{"id":"fXvelOH4-5ZU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754018630192,"user_tz":240,"elapsed":8,"user":{"displayName":"Devon Peticolas","userId":"01655597089936748753"}},"outputId":"8e8888f4-10ca-4b7f-8992-12fe533714bf"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Unique words: 579\n","Top 5 words: [(\"i'm\", 28), ('but', 20), ('like', 16), ('rap', 13), ('get', 12)]\n"]}]},{"cell_type":"markdown","source":["\n","# Part 5: Calculate Document Frequency\n","\n","Write a function that calculates how many documents (songs) the word appears in"],"metadata":{"id":"Hx7hpux11bDK"}},{"cell_type":"code","source":["def calculate_document_frequency(corpus, target_terms):\n","    doc_freq = {}\n","\n","    # FIRST, preprocess all of the corpus into a list of preprocessed list\n","    processed_corpus = []\n","    for doc in corpus:\n","        processed_corpus.append(preprocess(doc))\n","\n","    for term in target_terms:\n","        for doc in processed_corpus:\n","            if term in doc:\n","                doc_freq[term] = doc_freq.get(term, 0) + 1\n","\n","    return doc_freq\n","\n","# Create corpus and calculate DF\n","corpus = df[\"Lyric\"].tolist()\n","target_words = list(set(tf.keys()))  # Unique words from our target song\n","\n","print(f\"Calculating document frequency for {len(target_words)} words...\")\n","df_counts = calculate_document_frequency(corpus, target_words)\n","\n","for term, doc_freq in list(df_counts.items())[:10]:\n","    print(f\"{term}: {doc_freq}\")\n"],"metadata":{"id":"sQRuyvj7yBrt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754018652173,"user_tz":240,"elapsed":21983,"user":{"displayName":"Devon Peticolas","userId":"01655597089936748753"}},"outputId":"75436f23-4964-44e6-b3fe-d37494df35d6"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Calculating document frequency for 579 words...\n","zod: 2\n","wrong: 659\n","basically: 24\n","least: 156\n","mayweather's: 2\n","arm: 95\n","'cause: 2032\n","can't: 2068\n","crap: 32\n","it's: 2968\n"]}]},{"cell_type":"markdown","source":["# Part 6: Calculate TF-IDF\n","Write a function to calculate the final TF-IDF scores:\n"],"metadata":{"id":"fGfSmR2V1cxW"}},{"cell_type":"code","source":["def calculate_tfidf(term_freq, doc_freq, total_docs):\n","    tfidf = {}\n","\n","    for term in term_freq:\n","        tf = term_freq[term]\n","        idf = math.log(total_docs / doc_freq[term])\n","        tfidf[term] = tf * idf\n","\n","    return tfidf\n","\n","# Calculate TF-IDF\n","tfidf_scores = calculate_tfidf(tf, df_counts, len(corpus))\n","\n","# Display results\n","sorted_scores = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)\n","print(f\"\\nTop 20 most distinctive words in '{title}':\")\n","for word, score in sorted_scores[:20]:\n","    print(f\"  {word}: {score:.3f}\")"],"metadata":{"id":"BL3a4NJ9GuIc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754018652176,"user_tz":240,"elapsed":20,"user":{"displayName":"Devon Peticolas","userId":"01655597089936748753"}},"outputId":"dc4143d8-da78-497c-c4dc-e0227fbd8bdc"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Top 20 most distinctive words in 'Rap God':\n","  rap: 37.430\n","  nod: 37.269\n","  lookin': 31.204\n","  boy: 21.031\n","  god: 20.626\n","  beginnin': 19.497\n","  slap: 18.088\n","  asgard: 16.006\n","  box: 15.841\n","  doc: 15.423\n","  nascar: 14.174\n","  fab: 14.174\n","  minutes: 13.218\n","  i'm: 12.473\n","  front: 12.106\n","  motherfuckin': 11.702\n","  fame: 11.441\n","  satan: 11.401\n","  while: 11.317\n","  fuck: 11.008\n"]}]},{"cell_type":"markdown","source":["# Part 7: What did we learn?\n","Here's some code to help print out the most common words and the most distinctive words."],"metadata":{"id":"dWuKVNJ81suc"}},{"cell_type":"code","source":["print(\"\\nAnalysis:\")\n","print(f\"Tokens with highest TFIDF scores:\")\n","print(f\"Most common words:\")\n","for term, tf_score in sorted(tf.items(), key=lambda x: x[1], reverse=True)[:5]:\n","      print(f\"- {term}: {tf_score}\")\n","\n","print(f\"Most distinctive words:\")\n","for term, tfidf_score in sorted_scores[:5]:\n","      print(f\"- {term}: {tfidf_score}\")\n","\n","print(\"Are they the same? What does this tell us about TF-IDF?\")"],"metadata":{"id":"oi7PlOP16NTg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754018679683,"user_tz":240,"elapsed":12,"user":{"displayName":"Devon Peticolas","userId":"01655597089936748753"}},"outputId":"26291010-6a4b-46de-a955-58231a4d1262"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Analysis:\n","Tokens with highest TFIDF scores:\n","Most common words:\n","- i'm: 28\n","- but: 20\n","- like: 16\n","- rap: 13\n","- get: 12\n","Most distinctive words:\n","- rap: 37.43001466205759\n","- nod: 37.26861844353934\n","- lookin': 31.203939538982407\n","- boy: 21.03074593847605\n","- god: 20.62596687202567\n","Are they the same? What does this tell us about TF-IDF?\n"]}]}]}