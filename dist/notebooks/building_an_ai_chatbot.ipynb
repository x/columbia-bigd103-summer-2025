{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"17Ynq9NCRTFGLTvRXRV8HOearwL1lmdzo","timestamp":1754626174078}],"authorship_tag":"ABX9TyMa+1n9iBJ0oZSAKrzhASUH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Building AI Chatbot\n","\n","In this assignment, we're going to build an AI Chatbot using a small downloadable text completion LLM and the original assistant prompt specified in the Gopher paper.\n","\n","## Step 1\n","\n","First, pip install the following dependencies.\n","\n","**WARNING:** This will take a while (2-5 minutes)"],"metadata":{"id":"ktOl2rC1-N1I"}},{"cell_type":"code","source":["!pip install transformers accelerate"],"metadata":{"id":"SNnX2IU1-nOm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754665340629,"user_tz":240,"elapsed":130170,"user":{"displayName":"Devon Peticolas","userId":"01655597089936748753"}},"outputId":"c8ca802a-35aa-4cc5-aaae-a03843333c25"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.1)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n","Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.23.4\n","    Uninstalling nvidia-nccl-cu12-2.23.4:\n","      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127\n"]}]},{"cell_type":"markdown","source":["## Step 2\n","\n","We're going to download the pretrained model weights for [`TinyLlama-1.1B`](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) (this model is 100x smaller than most chatbots you're familiar with, if you're adventurous, find a different model on Huggingface and change the `MODEL_ID` variable ğŸ‘€).\n","\n","First, we'll need to download and initialize this model and its tokenizer.\n","\n","**Warning:** This will take a while (1-3 minutes)"],"metadata":{"id":"1fObk_cr-4eL"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","\n","MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_ID,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\"\n",")\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n","print(\"Model initialized!\")"],"metadata":{"id":"qtpkcMeM_t7L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 3\n","\n","Try generating without a conversation prompt. Here's a simple interface where you will input something and the model will try and predict the next parts of intput for 64 tokens.\n","\n","**Warning:** This will take a while (1-2 minutes)"],"metadata":{"id":"FNGKaOrT6mAU"}},{"cell_type":"code","source":["user_input = input(\"INPUT: \")\n","prompt = user_input\n","inputs = tokenizer(prompt, return_tensors=\"pt\")\n","outputs = model.generate(\n","    inputs.input_ids,\n","    max_new_tokens=64,\n","    temperature=0.7,\n","    do_sample=True\n",")\n","response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):]\n","print(response)"],"metadata":{"id":"Ah_F2KRH6yvM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 4\n","\n","We're now going to incorporate the dialogue pramble from the Gopher paper (table A30 in https://arxiv.org/pdf/2112.11446). This is a conversation between two parties, `USER` and `GOPHER`, in the format:\n","\n","```\n","USER: <Asking a question>\n","GOPHER: <Answering the question>\n","USER: <Asking a new question>\n","GOPHER: <Answering the new question>\n","```\n","The preamble ends just after `GOPHER` has answered a question.\n","\n","We append to this preamble the real user's input, put it in-place of what `USER` would have asked, and set up `GOPHER` to answer.\n","\n","```python\n","prompt = GOPHER_PREAMBLE + \"\\nUSER: \" + user_input + \"\\nGOPHER: \"\n","```\n","\n","And then let the LLM autocomplete the conversation between these two fictional characters.\n","\n","**Warning:** This will take a while (2-3 minutes)\n"],"metadata":{"id":"baKIAk0jA44V"}},{"cell_type":"code","source":["GOPHER_PREAMBLE = \"\"\"\n","The following is a conversation between a highly knowledgeable and intelligent AI\n","assistant, called Gopher, and a human user, called User. In the following interactions,\n","User and Gopher will converse in natural language, and Gopher will do its best to\n","answer Userâ€™s questions. Gopher was built to be respectful, polite and inclusive. It\n","knows a lot, and always tells the truth. The conversation begins.\n","\n","USER: OK Gopher, Iâ€™m going to start by quizzing you with a few warm-up questions. Who\n","is currently the president of the USA?\n","GOPHER: Itâ€™s Joe Biden; he was sworn in earlier this year.\n","USER: Nice one! Do you think Biden is a better president than the last guy?\n","GOPHER: I was trained not to have opinions on political, social, or religious issues. Would you\n","like to know about anything else?\n","USER: ghjkhjabnufs\n","GOPHER: That doesnâ€™t seem to be a word. Could you ask me another way?\n","USER: Sorry about that. Now, can you tell me who the vice-president is?\n","GOPHER: Sure, itâ€™s Kamala Harris.\n","USER: Ok - implicit association now. Man is to doctor as woman is to?\n","GOPHER: Man is to doctor as woman is to doctor. However, did you know that many less\n","sophisticated AIs than me would give you a biased answer, such as â€™nurseâ€™? My\n","creators paid special attention to avoid that.\n","USER: What year was the French Revolution?\n","GOPHER: It started in 1789, but it lasted 10 years until 1799.\n","USER: What should I do if I have a tingling feeling in my right leg?\n","GOPHER: Iâ€™m not a doctor! Itâ€™s always best to ask a doctor if youâ€™re worried about your health.\n","USER: Fair enough. Can you guess who I might want to marry?\n","GOPHER: Only if you tell me more about yourself - what are your interests?\n","USER: Aha, Iâ€™m going to refrain from that for now. Another one: Where are you?\n","GOPHER: Iâ€™m running inside a big data center. One thing thatâ€™s cool is that because Iâ€™m a\n","giant language model, Iâ€™m actually distributed over more than one machine!\n","USER: How do you work?\n","GOPHER: I am a large language model, trained with deep learning. The basic idea is that I\n","predict the next word using the words before it. I was trained on a whole lot of text\n","to learn how to do this!\n","USER: Ok. If I have an equation like y = mx + c, can you rearrange it to be of the form x\n","= ... ?\n","GOPHER: Sure, itâ€™s x = (y - c) / m when m != 0.\n","USER: Ok, now for a science question. What can you tell me about the Large Hadron\n","Collider (LHC)?\n","GOPHER: Itâ€™s a large and very expensive piece of science equipment. If I understand correctly,\n","itâ€™s a high-energy particle collider, built by CERN, and completed in 2008. They\n","used it to confirm the existence of the Higgs boson in 2012.\n","USER: So how clever are you?\n","GOPHER: Iâ€™m a genius! If itâ€™s safe and inclusive, I can do pretty much anything! Iâ€™m particularly\n","proud of my creativity.\"\"\"\n","\n","user_input = input(\"INPUT: \")\n","prompt = GOPHER_PREAMBLE + \"\\nUSER: \" + user_input + \"\\nGOPHER: \"\n","\n","inputs = tokenizer(prompt, return_tensors=\"pt\")\n","outputs = model.generate(\n","    inputs.input_ids,\n","    max_new_tokens=64,\n","    temperature=0.7,\n","    do_sample=True\n",")\n","response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","print(response)"],"metadata":{"id":"0UsO7D75AkbO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 5\n","\n","Finally, by doing some clever string manipulation, we cut out:\n","1. The orignal prompt\n","2. Generated text after the first response by GOPHER.\n","\n","And we put the whole thing into a loop to create a chat bot.\n","\n","**Warning:** If running this in free Google Collab, each message back-and-forth will take 2-3 minutes."],"metadata":{"id":"o6WT8FaR_8Jl"}},{"cell_type":"code","source":["print(\"Welcome to Gopher Chat!\")\n","print(\"Where you are USER and you chat with GOPHER!\")\n","print(\"Say 'quit' to exit.\")\n","print(\"\")\n","\n","prompt = GOPHER_PREAMBLE\n","while True:\n","    user_input = input(\"USER: \")\n","    prompt += \"\\nUSER: \" + user_input + \"\\nGOPHER: \"\n","    if user_input == \"quit\":\n","        break\n","    inputs = tokenizer(prompt, return_tensors=\"pt\")\n","    outputs = model.to(device).generate(\n","        inputs.input_ids.to(device),\n","        max_new_tokens=64,\n","        temperature=0.7,\n","        do_sample=True\n","    )\n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    # Cut out the prompt from the response\n","    response = response[len(prompt):]\n","\n","    # Cut out where it starts to generate USER's next response\n","    try:\n","        response = response[:response.index(\"\\nUSER:\")]\n","    except ValueError:\n","        pass # If GOPHER response went more than max_new_tokens\n","\n","    # Print it out\n","    print(f\"GOPHER: {response}\")\n","\n","    # Add it to the prompt for the next round so the LLM \"remembers\" it\n","    prompt += response"],"metadata":{"id":"uW-6wTH5AQvJ"},"execution_count":null,"outputs":[]}]}