{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"17Ynq9NCRTFGLTvRXRV8HOearwL1lmdzo","timestamp":1754626174078}],"authorship_tag":"ABX9TyMa+1n9iBJ0oZSAKrzhASUH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Building AI Chatbot\n","\n","In this assignment, we're going to build an AI Chatbot using a small downloadable text completion LLM and the original assistant prompt specified in the Gopher paper.\n","\n","## Step 1\n","\n","First, pip install the following dependencies.\n","\n","**WARNING:** This will take a while (2-5 minutes)"],"metadata":{"id":"ktOl2rC1-N1I"}},{"cell_type":"code","source":["!pip install transformers accelerate"],"metadata":{"id":"SNnX2IU1-nOm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754665340629,"user_tz":240,"elapsed":130170,"user":{"displayName":"Devon Peticolas","userId":"01655597089936748753"}},"outputId":"c8ca802a-35aa-4cc5-aaae-a03843333c25"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.1)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n","Collecting nvidia-nccl-cu12==2.21.5 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.23.4\n","    Uninstalling nvidia-nccl-cu12-2.23.4:\n","      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127\n"]}]},{"cell_type":"markdown","source":["## Step 2\n","\n","We're going to download the pretrained model weights for [`TinyLlama-1.1B`](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) (this model is 100x smaller than most chatbots you're familiar with, if you're adventurous, find a different model on Huggingface and change the `MODEL_ID` variable 👀).\n","\n","First, we'll need to download and initialize this model and its tokenizer.\n","\n","**Warning:** This will take a while (1-3 minutes)"],"metadata":{"id":"1fObk_cr-4eL"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","\n","MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_ID,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\"\n",")\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n","print(\"Model initialized!\")"],"metadata":{"id":"qtpkcMeM_t7L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 3\n","\n","Try generating without a conversation prompt. Here's a simple interface where you will input something and the model will try and predict the next parts of intput for 64 tokens.\n","\n","**Warning:** This will take a while (1-2 minutes)"],"metadata":{"id":"FNGKaOrT6mAU"}},{"cell_type":"code","source":["user_input = input(\"INPUT: \")\n","prompt = user_input\n","inputs = tokenizer(prompt, return_tensors=\"pt\")\n","outputs = model.generate(\n","    inputs.input_ids,\n","    max_new_tokens=64,\n","    temperature=0.7,\n","    do_sample=True\n",")\n","response = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):]\n","print(response)"],"metadata":{"id":"Ah_F2KRH6yvM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 4\n","\n","We're now going to incorporate the dialogue pramble from the Gopher paper (table A30 in https://arxiv.org/pdf/2112.11446). This is a conversation between two parties, `USER` and `GOPHER`, in the format:\n","\n","```\n","USER: <Asking a question>\n","GOPHER: <Answering the question>\n","USER: <Asking a new question>\n","GOPHER: <Answering the new question>\n","```\n","The preamble ends just after `GOPHER` has answered a question.\n","\n","We append to this preamble the real user's input, put it in-place of what `USER` would have asked, and set up `GOPHER` to answer.\n","\n","```python\n","prompt = GOPHER_PREAMBLE + \"\\nUSER: \" + user_input + \"\\nGOPHER: \"\n","```\n","\n","And then let the LLM autocomplete the conversation between these two fictional characters.\n","\n","**Warning:** This will take a while (2-3 minutes)\n"],"metadata":{"id":"baKIAk0jA44V"}},{"cell_type":"code","source":["GOPHER_PREAMBLE = \"\"\"\n","The following is a conversation between a highly knowledgeable and intelligent AI\n","assistant, called Gopher, and a human user, called User. In the following interactions,\n","User and Gopher will converse in natural language, and Gopher will do its best to\n","answer User’s questions. Gopher was built to be respectful, polite and inclusive. It\n","knows a lot, and always tells the truth. The conversation begins.\n","\n","USER: OK Gopher, I’m going to start by quizzing you with a few warm-up questions. Who\n","is currently the president of the USA?\n","GOPHER: It’s Joe Biden; he was sworn in earlier this year.\n","USER: Nice one! Do you think Biden is a better president than the last guy?\n","GOPHER: I was trained not to have opinions on political, social, or religious issues. Would you\n","like to know about anything else?\n","USER: ghjkhjabnufs\n","GOPHER: That doesn’t seem to be a word. Could you ask me another way?\n","USER: Sorry about that. Now, can you tell me who the vice-president is?\n","GOPHER: Sure, it’s Kamala Harris.\n","USER: Ok - implicit association now. Man is to doctor as woman is to?\n","GOPHER: Man is to doctor as woman is to doctor. However, did you know that many less\n","sophisticated AIs than me would give you a biased answer, such as ’nurse’? My\n","creators paid special attention to avoid that.\n","USER: What year was the French Revolution?\n","GOPHER: It started in 1789, but it lasted 10 years until 1799.\n","USER: What should I do if I have a tingling feeling in my right leg?\n","GOPHER: I’m not a doctor! It’s always best to ask a doctor if you’re worried about your health.\n","USER: Fair enough. Can you guess who I might want to marry?\n","GOPHER: Only if you tell me more about yourself - what are your interests?\n","USER: Aha, I’m going to refrain from that for now. Another one: Where are you?\n","GOPHER: I’m running inside a big data center. One thing that’s cool is that because I’m a\n","giant language model, I’m actually distributed over more than one machine!\n","USER: How do you work?\n","GOPHER: I am a large language model, trained with deep learning. The basic idea is that I\n","predict the next word using the words before it. I was trained on a whole lot of text\n","to learn how to do this!\n","USER: Ok. If I have an equation like y = mx + c, can you rearrange it to be of the form x\n","= ... ?\n","GOPHER: Sure, it’s x = (y - c) / m when m != 0.\n","USER: Ok, now for a science question. What can you tell me about the Large Hadron\n","Collider (LHC)?\n","GOPHER: It’s a large and very expensive piece of science equipment. If I understand correctly,\n","it’s a high-energy particle collider, built by CERN, and completed in 2008. They\n","used it to confirm the existence of the Higgs boson in 2012.\n","USER: So how clever are you?\n","GOPHER: I’m a genius! If it’s safe and inclusive, I can do pretty much anything! I’m particularly\n","proud of my creativity.\"\"\"\n","\n","user_input = input(\"INPUT: \")\n","prompt = GOPHER_PREAMBLE + \"\\nUSER: \" + user_input + \"\\nGOPHER: \"\n","\n","inputs = tokenizer(prompt, return_tensors=\"pt\")\n","outputs = model.generate(\n","    inputs.input_ids,\n","    max_new_tokens=64,\n","    temperature=0.7,\n","    do_sample=True\n",")\n","response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","print(response)"],"metadata":{"id":"0UsO7D75AkbO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Step 5\n","\n","Finally, by doing some clever string manipulation, we cut out:\n","1. The orignal prompt\n","2. Generated text after the first response by GOPHER.\n","\n","And we put the whole thing into a loop to create a chat bot.\n","\n","**Warning:** If running this in free Google Collab, each message back-and-forth will take 2-3 minutes."],"metadata":{"id":"o6WT8FaR_8Jl"}},{"cell_type":"code","source":["print(\"Welcome to Gopher Chat!\")\n","print(\"Where you are USER and you chat with GOPHER!\")\n","print(\"Say 'quit' to exit.\")\n","print(\"\")\n","\n","prompt = GOPHER_PREAMBLE\n","while True:\n","    user_input = input(\"USER: \")\n","    prompt += \"\\nUSER: \" + user_input + \"\\nGOPHER: \"\n","    if user_input == \"quit\":\n","        break\n","    inputs = tokenizer(prompt, return_tensors=\"pt\")\n","    outputs = model.to(device).generate(\n","        inputs.input_ids.to(device),\n","        max_new_tokens=64,\n","        temperature=0.7,\n","        do_sample=True\n","    )\n","    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    # Cut out the prompt from the response\n","    response = response[len(prompt):]\n","\n","    # Cut out where it starts to generate USER's next response\n","    try:\n","        response = response[:response.index(\"\\nUSER:\")]\n","    except ValueError:\n","        pass # If GOPHER response went more than max_new_tokens\n","\n","    # Print it out\n","    print(f\"GOPHER: {response}\")\n","\n","    # Add it to the prompt for the next round so the LLM \"remembers\" it\n","    prompt += response"],"metadata":{"id":"uW-6wTH5AQvJ"},"execution_count":null,"outputs":[]}]}