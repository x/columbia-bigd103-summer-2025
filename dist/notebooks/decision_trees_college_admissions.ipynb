{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Decision Trees: Predicting College Admissions\n","\n","In this exercise, we'll use decision trees to predict whether students get admitted to college based on their application data.\n","\n","## What is a Decision Tree?\n","\n","A decision tree is like a flowchart that asks yes/no questions about your data to make predictions. Unlike logistic regression which creates a smooth probability curve, decision trees create clear \"if-then\" rules.\n","\n","For example:\n","- If CGPA > 9.0, then predict \"Admitted\"\n","- Else if GRE Score > 320 AND Research = 1, then predict \"Admitted\"\n","- Else predict \"Not Admitted\""],"metadata":{"id":"nb_guNUwvNrW"}},{"cell_type":"markdown","source":["## Part 1: Load and Explore the Data\n"],"metadata":{"id":"J2kPQzllxFg2"}},{"cell_type":"code","source":["# JUST RUN THIS\n","\n","from google.colab import drive\n","import pandas as pd\n","\n","drive.mount('/content/gdrive')\n","\n","# Load the data\n","df = pd.read_csv('/content/gdrive/MyDrive/datasets/admission_predict.csv')\n","\n","# Convert \"Chance of Admit\" to a True/False \"Admitted\" column\n","df[\"Admitted\"] = df[\"Chance of Admit \"] > 0.75\n","df.drop(\"Chance of Admit \", axis=1, inplace=True)\n","df.rename(columns={\"LOR \": \"LOR\"}, inplace=True)\n","\n","# Explore the data\n","print(f\"Total applicants: {len(df)}\")\n","print(f\"Admitted: {df['Admitted'].sum()}\")\n","print(f\"Not admitted: {(~df['Admitted']).sum()}\")\n","print(\"\\nColumns:\")\n","print(df.columns.tolist())\n","\n","# Look at a random sample of the data\n","print(\"\\nRandom sample of 5 applicants:\")\n","df.sample(5)"],"metadata":{"id":"zoWR_jG5xG1U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 2: Train-Test Split\n","\n","Just like with logistic regression, we need to split our data for training and testing.\n"],"metadata":{"id":"Pj79rRfhxQxb"}},{"cell_type":"code","source":["# JUST RUN THIS\n","\n","from sklearn.model_selection import train_test_split\n","\n","# Split the data\n","df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n","\n","print(f\"Training set: {len(df_train)} applicants\")\n","print(f\"Test set: {len(df_test)} applicants\")"],"metadata":{"id":"d2cDsXY4xbA1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 3: Prepare Features and Labels\n","\n","Let's separate our features (X) from our labels (y).\n","\n","### HINTS\n","\n","This is exactly like the logistic regression assignment! We need to:\n","1. Select all columns except 'Serial No.' and 'Admitted' as features\n","2. Extract the 'Admitted' column as our labels\n","\n","Remember:\n","- For features (X), we drop the columns we don't want\n","- For labels (y), we select just the 'Admitted' column"],"metadata":{"id":"GZsCrxddxOiN"}},{"cell_type":"code","source":["def prepare_features(df_train, df_test):\n","    # Input: df_train and df_test are DataFrames with all columns\n","    # Output: Returns X_train, X_test (features only, no Serial No. or Admitted)\n","\n","    # TODO: Your code here!\n","    # 1. First, list all the feature columns you want to use\n","    #    Hint: All columns except 'Serial No.' and 'Admitted'\n","    #    You can type them out: feature_cols = ['GRE Score', 'TOEFL Score', ...]\n","    #    Or use: feature_cols = df_train.columns.drop(['Serial No.', 'Admitted']).tolist()\n","\n","    # 2. Create X_train using double brackets\n","    #    X_train = df_train[feature_cols]\n","\n","    # 3. Create X_test the same way\n","    #    X_test = df_test[feature_cols]\n","\n","    # 4. Return both (yes, functions can return multiple values!)\n","    #    return X_train, X_test\n","    pass\n","\n","def prepare_labels(df_train, df_test):\n","    # Input: df_train and df_test are DataFrames\n","    # Output: Returns y_train, y_test (just the Admitted column)\n","\n","    # TODO: Your code here!\n","    # 1. Extract the 'Admitted' column from df_train\n","    #    y_train = df_train['Admitted']  # Single brackets for a Series!\n","\n","    # 2. Extract the 'Admitted' column from df_test\n","    #    y_test = df_test['Admitted']\n","\n","    # 3. Return both values\n","    #    return y_train, y_test\n","    pass\n","\n","\n","# Test your function\n","X_train, X_test = prepare_features(df_train, df_test)\n","y_train, y_test = prepare_labels(df_train, df_test)\n","print(f\"Features shape - Train: {X_train.shape}, Test: {X_test.shape}\")\n","print(f\"Features used: {X_train.columns.tolist()}\")\n","print(f\"Training: {y_train.sum()} admitted out of {len(y_train)}\")\n","print(f\"Testing: {y_test.sum()} admitted out of {len(y_test)}\")"],"metadata":{"id":"4IBSguAoxpPR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 4: Train the Decision Tree Model\n","\n","Time to train our decision tree! This is very similar to logistic regression, but we use `DecisionTreeClassifier` instead.\n","\n","### HINTS\n","\n","The pattern is almost identical to logistic regression:\n","1. Import and create the model\n","2. Fit it to the training data\n","3. Return the trained model\n","\n","The only difference is we're using `DecisionTreeClassifier()` instead of `LogisticRegression()`."],"metadata":{"id":"M_Muw3_yxnOs"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","\n","def train_decision_tree(X_train, y_train):\n","    # Input: X_train (features), y_train (labels)\n","    # Output: Returns trained model\n","\n","    # TODO: Your code here!\n","    # 1. Create a DecisionTreeClassifier model\n","    #    model = DecisionTreeClassifier(random_state=42)\n","    #    Note: random_state ensures consistent results\n","\n","    # 2. Train the model using the fit method\n","    #    model.fit(X_train, y_train)\n","\n","    # 3. Return the trained model\n","    #    return model\n","    pass\n","\n","# Train the model\n","model = train_decision_tree(X_train, y_train)\n","print(\"Model trained!\")\n","print(f\"Tree depth: {model.get_depth()}\")\n","print(f\"Number of leaves: {model.get_n_leaves()}\")"],"metadata":{"id":"lGGmLCNsxv43"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 5: Visualize the Decision Tree\n","\n","One of the best things about decision trees is that we can see exactly how they make decisions!"],"metadata":{"id":"Visualization_Section"}},{"cell_type":"code","source":["# JUST RUN THIS\n","\n","from sklearn.tree import plot_tree\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(20, 10))\n","plot_tree(model,\n","          feature_names=X_train.columns,\n","          class_names=['Not Admitted', 'Admitted'],\n","          filled=True,\n","          max_depth=3)  # Only show first 3 levels for clarity\n","plt.title(\"Decision Tree for College Admissions (First 3 Levels)\")\n","plt.show()\n","\n","# What do you notice about which features appear at the top of the tree?"],"metadata":{"id":"Tree_Visualization"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 6: Make Predictions\n","\n","Now let's use our trained model to make predictions on the test set.\n","\n","### HINTS\n","\n","This is identical to the logistic regression assignment - just use `model.predict()`!"],"metadata":{"id":"sdNJeJq2xxiz"}},{"cell_type":"code","source":["def make_predictions(model, X_test):\n","    # Input: model (trained), X_test (features to predict)\n","    # Output: Returns predictions as a pandas Series\n","\n","    # TODO: Your code here!\n","    # 1. Use the model's predict method to get predictions\n","    #    y_pred = pd.Series(model.predict(X_test), index=X_test.index)\n","\n","    # 2. Return the Series\n","    #    return y_pred\n","    y_pred = pd.Series(model.predict(X_test), index=X_test.index)\n","    return y_pred\n","\n","# Make predictions\n","y_pred = make_predictions(model, X_test)\n","print(f\"Predicted {y_pred.sum()} admissions out of {len(y_pred)} applicants\")"],"metadata":{"id":"YFm0v4KOxzaj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 7: Evaluate Performance\n","\n","Let's calculate our model's accuracy. For decision trees, accuracy is often a good metric because they naturally handle class imbalance well but the following code calculates the full confusion matrix and all three metrics."],"metadata":{"id":"n0eajIiZx1Vu"}},{"cell_type":"code","source":["# JUST RUN THIS\n","\n","def calculate_confusion_matrix(y_test, y_pred):\n","    # Input: df has 'Admitted' and 'Predicted' columns\n","    # Output: Returns tp, tn, fp, fn\n","    tp = ((y_test == True)  & (y_pred == True)).sum()  # True Positive\n","    tn = ((y_test == False) & (y_pred == False)).sum() # True Negative\n","    fp = ((y_test == False) & (y_pred == True)).sum()  # False Positive\n","    fn = ((y_test == True)  & (y_pred == False)).sum() # False Negative\n","    return tp, tn, fp, fn\n","\n","# Calculate confusion matrix\n","tp, tn, fp, fn = calculate_confusion_matrix(y_test, y_pred)\n","print(\"                  Predicted Positive | Predicted Negative\")\n","print(f\"Actual Positive |{tp:>19d} |{fn:>19d} \")\n","print(f\"Actual Negative |{fp:>19d} |{tn:>19d} \")\n","print(\"\")\n","\n","# Calculate accuracy, precision, and recall\n","total = len(y_test)\n","accuracy = (tp + tn) / total\n","precision = tp / (tp + fp)\n","recall = tp / (tp + fn)\n","print(f\"Accuracy:  {accuracy:>6.2%} (Correctly classified {tp + tn} out of {total})\")\n","print(f\"Precision: {precision:>6.2%} (When predicted positive, correct {precision:.0%} of the time)\")\n","print(f\"Recall:    {recall:>6.2%} (Found {recall:.0%} of all positive cases)\")"],"metadata":{"id":"juGkP4fgx24x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 8: Feature Importance\n","\n","Unlike logistic regression, decision trees can tell us exactly how important each feature is!"],"metadata":{"id":"Feature_Importance_Section"}},{"cell_type":"code","source":["# JUST RUN THIS\n","\n","# Get feature importances\n","importances = pd.DataFrame({\n","    'Feature': X_train.columns,\n","    'Importance': model.feature_importances_\n","}).sort_values('Importance', ascending=False)\n","\n","print(\"Feature Importances (higher = more important):\")\n","print(importances)\n","\n","# Visualize feature importances\n","plt.figure(figsize=(10, 6))\n","plt.bar(importances['Feature'], importances['Importance'])\n","plt.xlabel('Features')\n","plt.ylabel('Importance')\n","plt.title('Feature Importances in Decision Tree')\n","plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"Feature_Importance_Code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bonus 1: Preventing Overfitting with Pruning\n","\n","Decision trees can grow very deep and memorize the training data. Let's create a simpler tree that generalizes better.\n","\n","### HINTS\n","\n","We can control tree complexity with parameters:\n","- `max_depth`: Maximum depth of the tree\n","- `min_samples_split`: Minimum samples needed to split a node\n","- `min_samples_leaf`: Minimum samples in a leaf node"],"metadata":{"id":"Pruning_Section"}},{"cell_type":"code","source":["def train_pruned_tree(X_train, y_train, max_depth=3):\n","    # Input: X_train, y_train, and max_depth parameter\n","    # Output: Returns a simpler, pruned decision tree\n","\n","    # TODO: Your code here!\n","    # 1. Create a DecisionTreeClassifier with max_depth limit\n","    #    model = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n","\n","    # 2. Train the model\n","    #    model.fit(X_train, y_train)\n","\n","    # 3. Return the model\n","    #    return model\n","    pass\n","\n","# Train a simpler tree\n","simple_model = train_pruned_tree(X_train, y_train, max_depth=3)\n","\n","# Evaluate the simpler model\n","\n","simple_predictions = pd.Series(make_predictions(simple_model, X_test))\n","\n","\n","tp, tn, fp, fn = calculate_confusion_matrix(y_test, simple_predictions)\n","total = len(y_test)\n","simple_accuracy = (tp + tn) / (tp + tn + fp + fn)\n","print(f\"Simple Tree (max_depth=3) Test Accuracy: {simple_accuracy:.2%}\")\n","print(f\"Original Tree Test Accuracy: {accuracy:.2%}\")\n","print(f\"\\nSimple tree has {simple_model.get_n_leaves()} leaves vs {model.get_n_leaves()} in original\")"],"metadata":{"id":"Pruning_Code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bonus 1.2: Visualize the Simpler Tree\n","\n","Let's see how much simpler our pruned tree is:"],"metadata":{"id":"Simple_Tree_Viz_Section"}},{"cell_type":"code","source":["# JUST RUN THIS\n","\n","plt.figure(figsize=(15, 8))\n","plot_tree(simple_model,\n","          feature_names=X_train.columns,\n","          class_names=['Not Admitted', 'Admitted'],\n","          filled=True,\n","          fontsize=10)\n","plt.title(\"Simplified Decision Tree (max_depth=3)\")\n","plt.show()\n","\n","# This tree is much easier to interpret!"],"metadata":{"id":"Simple_Tree_Viz_Code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bonus 2: Try Different Splitting Criteria\n","\n","Decision trees can use different methods to decide how to split:\n","- **Gini** (default): Measures impurity\n","- **Entropy**: Measures information gain\n","\n","Try creating a model with `criterion=\"entropy\"` and see if it performs differently!\n","\n","```python\n","entropy_model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, random_state=42)\n","entropy_model.fit(X_train, y_train)\n","\n","# Evaluate and compare\n","```"],"metadata":{"id":"Bonus_Splitting_Section"}},{"cell_type":"code","source":["# BONUS CODE HERE\n","\n"],"metadata":{"id":"Bonus_Splitting_Code"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bonus 3: Convert Tree to Python Code\n","\n","One amazing thing about decision trees is we can convert them to simple if-else statements!\n","\n","This code converts your trained tree into a Python function:"],"metadata":{"id":"Bonus_Convert_Section"}},{"cell_type":"code","source":["# JUST RUN THIS (if you're curious)\n","\n","from sklearn.tree import _tree\n","\n","def tree_to_code(tree, feature_names):\n","    tree_ = tree.tree_\n","    feature_name = [\n","        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n","        for i in tree_.feature\n","    ]\n","    print(\"def predict_admission({}):\".format(\", \".join(feature_names)))\n","\n","    def recurse(node, depth):\n","        indent = \"    \" * depth\n","        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n","            name = feature_name[node]\n","            threshold = tree_.threshold[node]\n","            print(\"{}if {} <= {:.2f}:\".format(indent, name, threshold))\n","            recurse(tree_.children_left[node], depth + 1)\n","            print(\"{}else:  # if {} > {:.2f}\".format(indent, name, threshold))\n","            recurse(tree_.children_right[node], depth + 1)\n","        else:\n","            # Get the class prediction\n","            values = tree_.value[node][0]\n","            class_idx = values.argmax()\n","            class_name = \"'Admitted'\" if class_idx == 1 else \"'Not Admitted'\"\n","            print(\"{}return {}\".format(indent, class_name))\n","\n","    recurse(0, 1)\n","\n","print(\"\\nYour simple decision tree as Python code:\")\n","print(\"=\" * 50)\n","tree_to_code(simple_model, X_train.columns)\n","print(\"\\n\" + \"=\" * 50)\n","print(\"You could copy this function and use it anywhere!\")"],"metadata":{"id":"Tree_To_Code"},"execution_count":null,"outputs":[]}]}